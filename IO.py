import os

import anndata as ad
import numpy as np
import polars as pl
from sklearn.preprocessing import LabelEncoder


def read_h5ad_datasets(directory: str) -> dict[str, ad.AnnData]:
    """Read all datasets inside a given folder and use a dictionary to store them

    Parameters
    ----------
    directory : str
        The directory where the Dataset folder is located

    Returns
    -------
    dict[str, ad.AnnData]
        A dictionary whose keys being the name of each dataset, values being the data of each dataset
    """
    return {
        os.path.splitext(file)[0]: ad.read_h5ad(os.path.join(root, file))
        for root, _, files in os.walk(directory)
        for file in files
        if file.endswith(".h5ad")
    }


def convert_dataset_dict_to_np_dict(
    dataset_dict: dict[str, ad.AnnData]
) -> dict[str, np.ndarray]:
    """Transform the data-structure of each dataset stored in the given dictionary from ad.AnnData to np.array,
    encode the `cell_type` stored in adata.obs["cell.type"] to numeric labels for each observation, and append these encoded values to the data

    Parameters
    ----------
    dataset_dict : dict[str, ad.AnnData]
        A dictionary whose keys being the name of each dataset, values being the data of each dataset

    Returns
    -------
    dict[str, np.ndarray]
        A dictionary transformed from the input dictionary, with the last column encoded from the `cell_type` of each observation
    """
    # TODO: Simplify this function, i.e. directly return
    np_dict = {}

    for dataset_name, adata in dataset_dict.items():
        label_encoder = LabelEncoder()
        encoded_values = label_encoder.fit_transform(adata.obs["cell.type"]).reshape(
            -1, 1
        )
        np_dict[dataset_name] = np.concatenate(
            (np.array(adata.X), encoded_values), axis=1
        )

    return dict(sorted(np_dict.items()))


def convert_dataset_dict_to_df_dict(
    dataset_dict: dict[str, ad.AnnData]
) -> dict[str, pl.DataFrame]:
    """Transform the data-structure of each dataset stored in the given dictionary from ad.AnnData to pl.DataFrame,
    get the `cell_type` stored in adata.obs["cell.type"] for each observation, and append these values to the data

    Parameters
    ----------
    dataset_dict : dict[str, ad.AnnData]
        A dictionary whose keys being the name of each dataset, values being the data of each dataset

    Returns
    -------
    dict[str, pl.DataFrame]
        A dictionary transformed from the input dictionary, with the last column being the `cell_type` of each observation
    """
    # TODO: Simplify this function, i.e. directly return
    df_dict = {}

    for dataset_name, adata in dataset_dict.items():
        data_dict = {
            **{var_name: adata.X[:, i] for i, var_name in enumerate(adata.var_names)},
            "cell_type": adata.obs["cell.type"],
        }
        df_dict[dataset_name] = pl.DataFrame(data_dict)

    return dict(sorted(df_dict.items()))


def get_data_dict(
    directory: str, returnType: str = "np.ndarray"
) -> dict[str, np.ndarray] | dict[str, pl.DataFrame]:
    """Read all datasets inside a given folder and use a dictionary to store them, user can choose the return data-structure of
    the stored data

    Parameters
    ----------
    directory : str
        The directory where the Dataset folder is located.
    returnType : str, optional
        The data-strcture of the data stored in the returned dictionary, by default "np.ndarray", can be either "np.ndarray" or "pl.DataFrame"

    Returns
    -------
    dict[str, np.ndarray] | dict[str, pl.DataFrame]
        A dictionary generated by either `convert_dataset_dict_to_np_dict()` or `convert_dataset_dict_to_df_dict`

    Raises
    ------
    ValueError
        When the user enter an invalid type
    """
    # TODO: More clever match method
    match returnType:
        case "np.ndarray":
            return convert_dataset_dict_to_np_dict(read_h5ad_datasets(directory))

        case "pl.DataFrame":
            return convert_dataset_dict_to_df_dict(read_h5ad_datasets(directory))

        case _:
            raise ValueError(
                "Please enter a legal dataType from [np.ndarry | pl.DataFrame]"
            )


# from deep_river.classification import RollingClassifier
# from river import metrics, compose, preprocessing
# import torch
#
#
# class MyModule(torch.nn.Module):
#
#     def __init__(self, n_features, hidden_size=1):
#         super().__init__()
#         self.n_features=n_features
#         self.hidden_size = hidden_size
#         self.lstm = torch.nn.LSTM(input_size=n_features, hidden_size=hidden_size,
#         batch_first=False,num_layers=1,bias=False)
#         self.softmax = torch.nn.Softmax(dim=-1)
#
#     def forward(self, X, **kwargs):
#         output, (hn, cn) = self.lstm(X)
#         hn = hn.view(-1, self.lstm.hidden_size)
#         return self.softmax(hn)
#
# metric = metrics.Accuracy()
# optimizer_fn = torch.optim.SGD
#
# model_pipeline = preprocessing.StandardScaler()
# model_pipeline |= RollingClassifier(
#     module=MyModule,
#     loss_fn="cross_entropy",
#     optimizer_fn=torch.optim.SGD,
#     window_size=20,
#     lr=1e-2,
#     append_predict=True,
#     is_class_incremental=True
# )
#
# for x, y in dataset.take(5000):
#     y_pred = model_pipeline.predict_one(x)  # make a prediction
#     metric = metric.update(y, y_pred)  # update the metric
#     model = model_pipeline.learn_one(x, y)  # make the model learn
#
# print(f'Accuracy: {metric.get()}')
